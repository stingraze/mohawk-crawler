#----------------------------------- 
# Files settings
#
# $seeds_file : file containing initial urls
# $db_file    : database file to create
# $db_layout  : how db records should be printed
# $max_description_length : max length of the description field
# $max_keywords_length    : max length of the keywords field
# $generate_keywords      : Generate automatically keywords from content if absent in meta
# $guess_encoding_problems     : Try do detect automatically character encoding problems
# $encoding_problems_threshold : Number between 0 and 1. Tolerance detect encoding problems
#----------------------------------- 
$seeds_file  = "seeds.txt";
$db_file     = "spider_#.dat";
$db_layout   = "##URL##\t##KEYWORDS##\t##DESCRIPTION##\t##TITLE##\n";
$max_description_length = 400;
$max_keywords_length    = 800;
$generate_keywords      = 1;
$guess_encoding_problems     = 0;
$encoding_problems_threshold = 0.1;

#----------------------------------- 
# DB size limiting. Contitions when crawler shoud stop running
#
# $max_iterations : max depth of navigation from a seed url;
# $max_db_size    : max number of lines in the db
#----------------------------------- 
# Files settings
#
# $seeds_file : file containing initial urls
# $db_file    : database file to create
# $db_layout  : how db records should be printed
# $max_description_length : max length of the description field
# $max_keywords_length    : max length of the keywords field
# $generate_keywords      : Generate automatically keywords from content if absent in meta
# $guess_encoding_problems     : Try do detect automatically character encoding problems
# $encoding_problems_threshold : Number between 0 and 1. Tolerance detect encoding problems
#----------------------------------- 
$seeds_file  = "seeds.txt";
$db_file     = "spider_#.dat";
$db_layout   = "##URL##\t##KEYWORDS##\t##DESCRIPTION##\t##TITLE##\n";
$max_description_length = 400;
$max_keywords_length    = 800;
$generate_keywords      = 1;
$guess_encoding_problems     = 0;
$encoding_problems_threshold = 0.1;
$domain_max_timeout     = 5;

#----------------------------------- 
# DB size limiting. Contitions when crawler shoud stop running
#
# $max_iterations : max depth of navigation from a seed url;
# $max_db_size    : max number of lines in the db
#----------------------------------- 
$max_iterations = 30;
$max_db_size    = 999999999;

#----------------------------------- 
# Include/exclude rules
#
# @exclude_url_rules  : collection of regular expressions. If an url
#                       matches one of these then it will not processed
# @include_url_rules  : collection of regular expressions. An url has to
#                       match one of these to be processed
# @exclude_content_types_rules : collection of regular expressions. If an
#                       url has a content type that matches one of these it will not processed
# @include_content_types_rules : collection of regular expressions. An url needs to have a content type
#                       that matches one of these to be processed
# $page_max_size:     : Max page size in bytes. If a page exceedes it won't be processed
# $ignore_robot_rules : Wether to ignore or not robots.txt file
#----------------------------------- 
@exclude_url_rules = ('statcounter');
@include_url_rules = ('^(http:\/\/www\.|https:\/\/www\.|http:\/\/|https:\/\/)?[a-z0-9]+([\-\.]{1}[a-z0-9]+)*\.[a-z]{2,5}(:[0-9]{1,5})?(\/.*)?$');
@exclude_content_types_rules = ();
@include_content_types_rules = ('text\/html', 'text\/plain');
$page_max_size = 22000000;
$ignore_robot_rules = 0;

#----------------------------------- 
# Connection properties
#
# $fetch_timeout   : connection timeout in seconds
# $user_agent      : the user agent string the spider will present to servers
# $max_redirects   : how many redirects to follow
# $fetch_pause     : seconds to wait between two consecutive page fetch
#----------------------------------- 
$fetch_timeout = 10;
$user_agent = "Mohawk Crawler v.1.00";
$max_redirects = 7;
$fetch_pause = 0.1;

#----------------------------------- 
# Proxy settings
# 
# $use_proxy  : set to 1 if connection is through a HTTP proxy
# $proxy_url  : url of the proxy server
#----------------------------------- 
$use_proxy = 0;
$proxy_url = 'http://us-dc.proxymesh.com:31280';

#----------------------------------- 
# Output settings
#
# $lines_per_file  : set max number of lines in output file
#-----------------------------------
$lines_per_file = 50000;
